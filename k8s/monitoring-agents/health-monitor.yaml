---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: health-monitor
  namespace: default
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: health-monitor
rules:
  - apiGroups: [""]
    resources: ["pods", "nodes", "services", "endpoints", "componentstatuses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments", "statefulsets", "daemonsets", "replicasets"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: health-monitor
subjects:
  - kind: ServiceAccount
    name: health-monitor
    namespace: default
roleRef:
  kind: ClusterRole
  name: health-monitor
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: health-monitor-script
  namespace: default
data:
  monitor.sh: |
    #!/bin/bash
    set -e

    echo "=== Cluster Health Monitor Report ==="
    echo "Timestamp: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
    echo ""

    # Check node health
    echo "--- Node Health ---"
    kubectl get nodes -o wide
    echo ""

    # Check component status
    echo "--- Component Status ---"
    kubectl get componentstatuses 2>/dev/null || echo "Component status API not available"
    echo ""

    # Check pod health across all namespaces
    echo "--- Pod Health Summary ---"
    kubectl get pods --all-namespaces -o wide | grep -v "Running\|Completed" || echo "All pods are healthy"
    echo ""

    # Check evolved-todo application health
    echo "--- Evolved Todo Application Health ---"
    kubectl get pods,services,deployments -n default -l app.kubernetes.io/instance=evolved-todo -o wide
    echo ""

    # Check for pods with restarts
    echo "--- Pods with Restarts ---"
    kubectl get pods --all-namespaces -o json | \
      jq -r '.items[] | select(.status.containerStatuses[]?.restartCount > 0) | "\(.metadata.namespace)/\(.metadata.name): \(.status.containerStatuses[].restartCount) restarts"' || \
      echo "No pods with restarts found"
    echo ""

    # Check resource usage
    echo "--- Resource Usage ---"
    kubectl top nodes 2>/dev/null || echo "Metrics server not available"
    kubectl top pods --all-namespaces 2>/dev/null | head -20 || echo "Pod metrics not available"
    echo ""

    # Check for pending pods
    echo "--- Pending Pods ---"
    kubectl get pods --all-namespaces --field-selector=status.phase=Pending || echo "No pending pods"
    echo ""

    echo "=== Health Monitor Report Complete ==="
---
apiVersion: batch/v1
kind: CronJob
metadata:
  name: health-monitor
  namespace: default
  labels:
    app: health-monitor
    component: monitoring
spec:
  schedule: "*/15 * * * *"  # Run every 15 minutes
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 3
  jobTemplate:
    spec:
      template:
        metadata:
          labels:
            app: health-monitor
        spec:
          serviceAccountName: health-monitor
          restartPolicy: OnFailure
          containers:
          - name: monitor
            image: bitnami/kubectl:latest
            imagePullPolicy: IfNotPresent
            command:
              - /bin/bash
              - /scripts/monitor.sh
            volumeMounts:
              - name: script
                mountPath: /scripts
            resources:
              requests:
                memory: "64Mi"
                cpu: "100m"
              limits:
                memory: "128Mi"
                cpu: "200m"
          volumes:
            - name: script
              configMap:
                name: health-monitor-script
                defaultMode: 0755
